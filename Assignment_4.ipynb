{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38583105",
   "metadata": {},
   "source": [
    "# Assignment 4: Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8635fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence, pad_sequence, pack_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import os\n",
    "\n",
    "# Below helps to run tokenizer with multiprocessing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6024f",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f182b757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis is the example of vectorization of dot product for two different sequence length.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m e_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      6\u001b[0m d_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      8\u001b[0m dot_product \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(e_states, d_states\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is the example of vectorization of dot product for two different sequence length.\n",
    "'''\n",
    "\n",
    "e_states = torch.randn(10, 100, 16)\n",
    "d_states = torch.randn(10, 80, 16)\n",
    "\n",
    "dot_product = torch.bmm(e_states, d_states.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "661d2571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[3.2446e-05, 3.0685e-05, 3.2470e-05,  ..., 3.1739e-05, 3.2247e-05,\n",
       "         3.0203e-05],\n",
       "        [3.1138e-05, 3.0794e-05, 3.1492e-05,  ..., 3.1710e-05, 3.2674e-05,\n",
       "         3.0614e-05],\n",
       "        [3.1536e-05, 3.1345e-05, 3.1711e-05,  ..., 3.1519e-05, 3.2388e-05,\n",
       "         2.9582e-05],\n",
       "        ...,\n",
       "        [3.3579e-05, 3.2189e-05, 2.8717e-05,  ..., 3.1391e-05, 3.2227e-05,\n",
       "         3.0350e-05],\n",
       "        [3.3719e-05, 3.1734e-05, 2.9376e-05,  ..., 3.1516e-05, 3.2870e-05,\n",
       "         3.0839e-05],\n",
       "        [3.3823e-05, 3.1278e-05, 3.0300e-05,  ..., 3.1402e-05, 3.3366e-05,\n",
       "         3.1277e-05]], grad_fn=<SoftmaxBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 63, 61, 58, 56, 53, 51, 50, 49, 47, 46, 45,\n",
       "        44, 42, 40, 38, 36, 36, 34, 34, 32, 32, 30, 29, 29, 28, 28, 27, 24, 22,\n",
       "        20, 17, 16, 16, 16, 15, 14, 13, 10, 10,  9,  9,  8,  5,  5,  4,  4,  4,\n",
       "         4,  3,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=tensor([57, 41,  1, 31, 58, 33, 28, 27, 22, 56, 63, 60, 20, 45,  4, 49, 35, 24,\n",
       "         6, 18, 51,  9, 19, 25, 37,  2, 10,  0, 48, 44, 50, 61, 32, 21, 14, 52,\n",
       "        55, 30, 17,  8, 59,  5, 38,  3, 62, 53, 11, 34, 23, 40, 47, 12,  7, 29,\n",
       "        15, 13, 43, 36, 42, 39, 16, 46, 54, 26]), unsorted_indices=tensor([27,  2, 25, 43, 14, 41, 18, 52, 39, 21, 26, 46, 51, 55, 34, 54, 60, 38,\n",
       "        19, 22, 12, 33,  8, 48, 17, 23, 63,  7,  6, 53, 37,  3, 32,  5, 47, 16,\n",
       "        57, 24, 42, 59, 49,  1, 58, 56, 29, 13, 61, 50, 28, 15, 30, 20, 35, 45,\n",
       "        62, 36,  9,  0,  4, 40, 11, 31, 44, 10]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionTranslator(Translator):\n",
    "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=256):\n",
    "    super().__init__(src_tokenizer, tgt_tokenizer, hidden_size)\n",
    "    self.decoder_proj = nn.Linear(hidden_size * 2, tgt_tokenizer.vocab_size)\n",
    "  \n",
    "  def get_attention(self, e_states, d_states, mask):\n",
    "    '''\n",
    "    Arguments:\n",
    "      e_states (torch.Tensor or torch.nn.utils.rnn.PackedSequence): Encoder's hidden states over timestep of source sentence. Has shape of [N x Ts x C]\n",
    "      d_states (torch.Tensor or torch.nn.utils.rnn.PackedSequence): Decoder's hidden states over timestep of target sentence. Has shape of [N x Tt x C]\n",
    "      mask (torch.Tensor): Boolean Tensor with a shape of [N x Ts]. The value of mask is True only if mask[n, t] is a padded value and otherwise False.\n",
    "    \n",
    "      N is number of sequence in the batch (batch size)\n",
    "      Ts is (maximum) length of source sentence (number of tokens)\n",
    "      Tt is (maximum) length of target sentence (number of tokens)\n",
    "      C is a hidden size\n",
    "       \n",
    "    Output:\n",
    "      weighted_sum (torch.Tensor or torch.nn.utils.rnn.PackedSequence):\n",
    "    \n",
    "    Caution:\n",
    "      e_states and d_states has different number of timeteps.\n",
    "      The attention score (dot product value) has to have a shape of [N, Ts, Tt].\n",
    "      \n",
    "      \n",
    "      \n",
    "    \n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "    is_packed = isinstance(e_states, PackedSequence)\n",
    "    if is_packed:\n",
    "      e_states, x_batch_len = pad_packed_sequence(e_states, batch_first=True)\n",
    "      d_states, y_batch_len = pad_packed_sequence(d_states, batch_first=True)\n",
    "    \n",
    "    '''\n",
    "    Write your code from here\n",
    "    '''\n",
    "    \n",
    "    e_states = e_states.unsqueeze(2)\n",
    "    d_states = d_states.unsqueeze(1)\n",
    "\n",
    "    attention_score = (e_states * d_states).sum(-1)\n",
    "    attention_score[mask] = -1e10\n",
    "    \n",
    "    attention_weight = torch.softmax(attention_score, dim=1)\n",
    "    weighted_sum = (e_states * attention_weight.unsqueeze(-1)).sum(1)\n",
    "    \n",
    "    '''\n",
    "    Leave the codes below.\n",
    "    '''\n",
    "    if is_packed:\n",
    "      weighted_sum = pack_padded_sequence(weighted_sum, y_batch_len, batch_first=True, enforce_sorted=False)\n",
    "    return weighted_sum\n",
    "  \n",
    "  def forward(self, x, y):\n",
    "    e_out_by_time, e_last_hidden  = self.run_encoder(x)\n",
    "    d_out_by_time = self.run_decoder(y, e_last_hidden)\n",
    "    \n",
    "    mask = pad_packed_sequence(x, batch_first=True)[0] == 0\n",
    "    \n",
    "    weighted_sum = self.get_attention(e_out_by_time, d_out_by_time, mask)\n",
    "    \n",
    "    if isinstance(y, PackedSequence):\n",
    "      logit = self.decoder_proj(torch.cat([weighted_sum.data, d_out_by_time.data], dim=-1))\n",
    "    else:\n",
    "      logit = self.decoder_proj(d_out_by_time)  # Num_batch x Num_timesteps x Num_vocab\n",
    "    \n",
    "    probs = torch.softmax(logit, dim=-1)\n",
    "    \n",
    "    if isinstance(y, PackedSequence):\n",
    "      out_seq = PackedSequence(probs, \n",
    "                               batch_sizes=y.batch_sizes, \n",
    "                               sorted_indices=y.sorted_indices, \n",
    "                               unsorted_indices=y.unsorted_indices)\n",
    "      return out_seq\n",
    "    else:\n",
    "      return probs\n",
    "    \n",
    "\n",
    "model = AttentionTranslator(src_tokenizer, tgt_tokenizer, 512)\n",
    "\n",
    "model(batch[0], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5213c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionTranslator(src_tokenizer, tgt_tokenizer, 512)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "trainer = Trainer(model, optimizer, get_cross_entropy_loss, train_loader, valid_loader, 'cuda', 'nmt_attention_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "67ec78f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f983a465f3471897c1b3cedb2684d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f965dd9c89224e3daf88d3b72028a5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.47 GiB (GPU 0; 23.69 GiB total capacity; 6.45 GiB already allocated; 2.92 GiB free; 9.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [154]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_by_num_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [152]\u001b[0m, in \u001b[0;36mTrainer.train_by_num_epoch\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 27\u001b[0m   loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_by_single_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_loss\u001b[38;5;241m.\u001b[39mappend(loss_value)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "Input \u001b[0;32mIn [152]\u001b[0m, in \u001b[0;36mTrainer._train_by_single_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     60\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(src\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), tgt_i\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(pred\u001b[38;5;241m.\u001b[39mdata, tgt_o\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_clip)\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.47 GiB (GPU 0; 23.69 GiB total capacity; 6.45 GiB already allocated; 2.92 GiB free; 9.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train_by_num_epoch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "efe50d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6b043148e0>]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainer.training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfefeae",
   "metadata": {},
   "source": [
    "## Problem 1: Implement Dot Product Attention\n",
    "\n",
    "- Optimizing computation time is really important\n",
    "    - Use `torch.mm()` or `torch.matmul()`\n",
    "    - `torch.mm(a, b)` is a function for calculating matrix multiplcation of two matrices `a` and `b`\n",
    "        - `a` and `b` has to be 2-dim tensors\n",
    "        - `a.shape[1]` has to be equal to `b.shape[0]`\n",
    "    - `torch.matmul()` is a function for matrix multiplication but with broadcasting\n",
    "        - https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "        - It has less restriction on its input shape.\n",
    "            - It automatically matches the dimension of two tensors following some rules\n",
    "            - Therefore, it is a bit risky to use this funciton if you don't understand how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059e903",
   "metadata": {},
   "source": [
    "### Hint: Dot product as matrix multiplcation.\n",
    "\n",
    "- Let's say there are two vector, $u=\\begin{bmatrix}-3 \\\\ 2 \\\\ 1\\end{bmatrix}$ and $v = \\begin{bmatrix} 5 \\\\ 4 \\\\ 6\\end{bmatrix}$\n",
    "    - The dot product of the two vectors is $(-3 \\times 5) + (2 \\times 4) + (1 \\times 6) = 1$\n",
    "    - It is equivalent to $u^T \\times v$\n",
    "        - In this case $u\\in\\mathbb{R}^{3x1}$ and $v\\in\\mathbb{R}^{3x1}$\n",
    "- In PyTorch, this can be described as below:\n",
    "    - `u = torch.Tensor([-3, 2, 1])`\n",
    "    - `v = torch.Tensor([5, 4, 6])`\n",
    "    - Dot product of u and v can be calculated by one of belows:\n",
    "        - `torch.mm(u.unsqueeze(0), v.unsqueeze(1))`\n",
    "            - `u.unsqueeze(0).shape == [1, 3]`\n",
    "            - `v.unsqueeze(1).shape == [3, 1]`\n",
    "            - `unsqueeze()` returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "            - The result has shape of [1,1]\n",
    "        - `torch.matmul(u, v)`\n",
    "        - `u @ v`\n",
    "            - `@` denotes matrix multiplication, which was introduced from Python 3.5\n",
    "        - `(u * v).sum()`\n",
    "            - This will be much slower than others, because it first do element-wise multiplcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4aed8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of (u * v).sum() is -1.0. This computation is much slower than others because it use element-wise multiplication instead of matrix multiplication\n",
      "Result of torch.mm(u.unsqueeze(0), v.unsqueeze(1)) is tensor([[-1.]])\n",
      "Result of torch.matmul(u, v) is -1.0\n",
      "Result of u @ v is -1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Hint: Dot product as matrix multiplcation.\n",
    "'''\n",
    "\n",
    "u = torch.Tensor([-3, 2, 1])\n",
    "v = torch.Tensor([5, 4, 6])\n",
    "\n",
    "print(f\"Result of (u * v).sum() is {(u * v).sum()}. This computation is much slower than others because it use element-wise multiplication instead of matrix multiplication\") \n",
    "print(f\"Result of torch.mm(u.unsqueeze(0), v.unsqueeze(1)) is {torch.mm(u.unsqueeze(0), v.unsqueeze(1))}\")\n",
    "print(f\"Result of torch.matmul(u, v) is {torch.matmul(u, v)}\")\n",
    "print(f\"Result of u @ v is {u @ v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a20b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_score_for_a_single_query(keys, query):\n",
    "  '''\n",
    "  This function returns an attention score for each vector in keys for a given query.\n",
    "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
    "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
    "  \n",
    "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
    "  \n",
    "  Arguments:\n",
    "    keys (torch.Tensor): Has a shape of [T, C]. These are vectors that a query wants attend to\n",
    "    query (torch.Tensor): Has a shape of [C]. This is a vector that attends to other set of vectors (keys and values)\n",
    "  \n",
    "  Output:\n",
    "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
    "                                    Has a shape of [T]\n",
    "                                    \n",
    "    attention_score[i] has to be a dot product value between keys[i] and query                                 \n",
    "\n",
    "\n",
    "  TODO: Complete this sentence using torch.mm (matrix multiplication)\n",
    "  Hint: You can use atensor.unsqueeze(dim) to expand a dimension (with a diemsion of length 1) without changing item value of the tensor.\n",
    "  '''\n",
    "  \n",
    "  return\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_t = 23\n",
    "h_size = 16\n",
    "\n",
    "keys = torch.randn(num_t, h_size)\n",
    "query = torch.randn(h_size)\n",
    "\n",
    "att_score = get_attention_score_for_a_single_query(keys, query)\n",
    "att_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce66bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all the cases!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Case\n",
    "'''\n",
    "assert att_score.ndim == 1 and len(att_score) == num_t, \"Error: Check output shape\"\n",
    "answer = torch.Tensor([-3.0786,  2.1729,  1.7950, -5.0503,  3.3254,  0.2828, -0.9800, -1.8868,\n",
    "         0.2550,  2.9389, -0.1799, -1.0586,  0.1465, -0.9441,  0.8888, -3.8108,\n",
    "        -2.5662, -1.1660, -2.2327,  2.7087, -0.5800,  8.7984,  4.3816])\n",
    "assert torch.max(torch.abs(att_score-answer)) < 1e-4, \"Error: The output value is different\"\n",
    "print(\"Passed all the cases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63d332d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m attention_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m att_weight \u001b[38;5;241m=\u001b[39m \u001b[43mget_attention_weight_from_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m att_weight\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mget_attention_weight_from_score\u001b[0;34m(attention_score)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_attention_weight_from_score\u001b[39m(attention_score):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m  This function converts attention score to attention weight.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m  \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m  TODO: Complete this function\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m  '''\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mattention_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "def get_attention_weight_from_score(attention_score):\n",
    "  '''\n",
    "  This function converts attention score to attention weight.\n",
    "  \n",
    "  Argument:\n",
    "    attention_score (torch.Tensor): Tensor of real number. Has a shape of [T]\n",
    "\n",
    "  Output:\n",
    "    attention_weight (torch.Tensor): Tensor of real number between 0 and 1. Sum of attention_weight is 1. Has a shape of [T]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  assert attention_score.ndim == 1\n",
    "  \n",
    "  return\n",
    "\n",
    "att_weight = get_attention_weight_from_score(att_score)\n",
    "att_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8030b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all the cases\n"
     ]
    }
   ],
   "source": [
    "answer = torch.Tensor([0.0000,     0.0013,     0.0009,     0.0000,     0.0041,     0.0002,\n",
    "            0.0001,     0.0000,     0.0002,     0.0028,     0.0001,     0.0001,\n",
    "            0.0002,     0.0001,     0.0004,     0.0000,     0.0000,     0.0000,\n",
    "            0.0000,     0.0022,     0.0001,     0.9756,     0.0118])\n",
    "assert att_weight.shape == att_score.shape, 'Shape has to be remained the same'\n",
    "assert att_weight.sum() == 1, \"Sum of attention weight has to be 1\"\n",
    "assert torch.max(torch.abs(att_weight-answer)) < 1e-4, \"Error: The output value is different\"\n",
    "\n",
    "print(\"Passed all the cases!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6bf8e701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
       "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weighted_sum(values, attention_weight):\n",
    "  '''\n",
    "  This function converts attention score to attention weight\n",
    "  \n",
    "  Argument:\n",
    "    values (torch.Tensor): Has a shape of [T, C]. These are vectors that are used to form attention vector\n",
    "    attention_weight: Has a shape of [T], which represents the weight for each vector to compose the attention vector\n",
    "\n",
    "  Output:\n",
    "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight. Has a shape of [C]\n",
    "  \n",
    "  TODO: Complete this function using torch.mm\n",
    "  '''\n",
    "  return\n",
    "\n",
    "att_vec = get_weighted_sum(keys, att_weight) # In simple dot-product-attention, key and value are the same\n",
    "att_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1253844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all the cases\n"
     ]
    }
   ],
   "source": [
    "answer = torch.Tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
    "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])\n",
    "assert att_vec.shape == query.shape, 'Shape has to be remained the same'\n",
    "assert torch.max(torch.abs(att_vec-answer)) < 1e-4, \"Error: The output value is different\"\n",
    "print(\"Passed all the cases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00df77",
   "metadata": {},
   "source": [
    "## Problem 2: Attention in Batch\n",
    "- In this problem, you have to calculate attention with batch\n",
    "- You can use `torch.bmm()` for batch matrix multiplication https://pytorch.org/docs/stable/generated/torch.bmm.html \n",
    "    - `torch.bmm()` takes two 3-dim tensor as its input\n",
    "    - Each tensor has to be 3-dim (atensor.ndim==3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "062446fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix_left1: \n",
      "tensor([[ 1.5410, -0.2934, -2.1788],\n",
      "        [ 0.5684, -1.0845, -1.3986],\n",
      "        [ 0.4033,  0.8380, -0.7193],\n",
      "        [-0.4033, -0.5966,  0.1820],\n",
      "        [-0.8567,  1.1006, -1.0712]])\n",
      "matrix_left2: \n",
      "tensor([[ 0.1227, -0.5663,  0.3731],\n",
      "        [-0.8920, -1.5091,  0.3704],\n",
      "        [ 1.4565,  0.9398,  0.7748],\n",
      "        [ 0.1919,  1.2638, -1.2904],\n",
      "        [-0.7911, -0.0209, -0.7185]])\n",
      "matrix_right1: \n",
      "tensor([[ 0.5186, -1.3125,  0.1920,  0.5428],\n",
      "        [-2.2188,  0.2590, -1.0297, -0.5008],\n",
      "        [ 0.2734, -0.9181, -0.0404,  0.2881]])\n",
      "matrix_right2: \n",
      "tensor([[-0.0075, -0.9145, -1.0886, -0.2666],\n",
      "        [ 0.1894, -0.2190,  2.0576, -0.0354],\n",
      "        [ 0.0627, -0.7663,  1.0993,  2.7565]])\n",
      "Let's assume that we have batch of matrix, which is stack of these two matices\n",
      "matrix_left: \n",
      "tensor([[[ 1.5410, -0.2934, -2.1788],\n",
      "         [ 0.5684, -1.0845, -1.3986],\n",
      "         [ 0.4033,  0.8380, -0.7193],\n",
      "         [-0.4033, -0.5966,  0.1820],\n",
      "         [-0.8567,  1.1006, -1.0712]],\n",
      "\n",
      "        [[ 0.1227, -0.5663,  0.3731],\n",
      "         [-0.8920, -1.5091,  0.3704],\n",
      "         [ 1.4565,  0.9398,  0.7748],\n",
      "         [ 0.1919,  1.2638, -1.2904],\n",
      "         [-0.7911, -0.0209, -0.7185]]]) \n",
      " which is shape of torch.Size([2, 5, 3])\n",
      "matrix_right: \n",
      "tensor([[[ 0.5186, -1.3125,  0.1920,  0.5428],\n",
      "         [-2.2188,  0.2590, -1.0297, -0.5008],\n",
      "         [ 0.2734, -0.9181, -0.0404,  0.2881]],\n",
      "\n",
      "        [[-0.0075, -0.9145, -1.0886, -0.2666],\n",
      "         [ 0.1894, -0.2190,  2.0576, -0.0354],\n",
      "         [ 0.0627, -0.7663,  1.0993,  2.7565]]])\n",
      " which is shape of torch.Size([2, 3, 4])\n",
      "mat_mul_stack: \n",
      "tensor([[[ 0.8547, -0.0982,  0.6861,  0.3556],\n",
      "         [ 2.3188,  0.2571,  1.2824,  0.4487],\n",
      "         [-1.8468,  0.3480, -0.7564, -0.4080],\n",
      "         [ 1.1644,  0.2078,  0.5296,  0.1323],\n",
      "         [-3.1791,  2.3929, -1.2545, -1.3247]],\n",
      "\n",
      "        [[-0.0848, -0.2741, -0.8887,  1.0159],\n",
      "         [-0.2559,  0.8624, -1.7270,  1.3123],\n",
      "         [ 0.2156, -2.1316,  1.2000,  1.7143],\n",
      "         [ 0.1570,  0.5366,  0.9730, -3.6531],\n",
      "         [-0.0431,  1.2786,  0.0284, -1.7689]]])\n",
      " which is shape of torch.Size([2, 5, 4])\n",
      "mat_mul_bmm: \n",
      "tensor([[[ 0.8547, -0.0982,  0.6861,  0.3556],\n",
      "         [ 2.3188,  0.2571,  1.2824,  0.4487],\n",
      "         [-1.8468,  0.3480, -0.7564, -0.4080],\n",
      "         [ 1.1644,  0.2078,  0.5296,  0.1323],\n",
      "         [-3.1791,  2.3929, -1.2545, -1.3247]],\n",
      "\n",
      "        [[-0.0848, -0.2741, -0.8887,  1.0159],\n",
      "         [-0.2559,  0.8624, -1.7270,  1.3123],\n",
      "         [ 0.2156, -2.1316,  1.2000,  1.7143],\n",
      "         [ 0.1570,  0.5366,  0.9730, -3.6531],\n",
      "         [-0.0431,  1.2786,  0.0284, -1.7689]]])\n",
      " which is shape of torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Hint for Problem 2\n",
    "\n",
    "You can calculate matrix multiplication of matrices in batch effectively using torch.bmm() or torch.matmul()\n",
    "'''\n",
    "\n",
    "torch.manual_seed(0)\n",
    "matrix_left1 = torch.randn(5, 3)\n",
    "matrix_left2 = torch.randn(5, 3)\n",
    "\n",
    "print(f\"matrix_left1: \\n{matrix_left1}\")\n",
    "print(f\"matrix_left2: \\n{matrix_left2}\")\n",
    "\n",
    "matrix_right1 = torch.randn(3, 4)\n",
    "matrix_right2 = torch.randn(3, 4)\n",
    "print(f\"matrix_right1: \\n{matrix_right1}\")\n",
    "print(f\"matrix_right2: \\n{matrix_right2}\")\n",
    "\n",
    "print(\"Let's assume that we have batch of matrix, which is stack of these two matices\")\n",
    "matrix_left = torch.stack([matrix_left1, matrix_left2])\n",
    "matrix_right = torch.stack([matrix_right1, matrix_right2])\n",
    "\n",
    "print(f\"matrix_left: \\n{matrix_left} \\n which is shape of {matrix_left.shape}\")\n",
    "print(f\"matrix_right: \\n{matrix_right}\\n which is shape of {matrix_right.shape}\")\n",
    "\n",
    "\n",
    "'''\n",
    "Exhaustive method: using torch.mm() only with for loop (This is SLOW when matrix gets much larger)\n",
    "'''\n",
    "\n",
    "mm_forloop_output = []\n",
    "for sample_index in range(matrix_left.shape[0]):\n",
    "  mat_left = matrix_left[sample_index]\n",
    "  mat_right = matrix_right[sample_index]\n",
    "  \n",
    "  mm_result = torch.mm(mat_left, mat_right)\n",
    "  mm_forloop_output.append(mm_result)\n",
    "  \n",
    "mm_forloop_stack = torch.stack(mm_forloop_output)\n",
    "print(f\"mat_mul_stack: \\n{mm_forloop_stack}\\n which is shape of {mm_forloop_stack.shape}\")\n",
    "\n",
    "\n",
    "'''\n",
    "Good method: using torch.bmm()\n",
    "'''\n",
    "\n",
    "mat_mul_bmm = torch.bmm(matrix_left, matrix_right)\n",
    "print(f\"mat_mul_bmm: \\n{mat_mul_bmm}\\n which is shape of {mat_mul_bmm.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe689641",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m query \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_b, h_size)\n\u001b[1;32m     32\u001b[0m out \u001b[38;5;241m=\u001b[39m get_attention_score_for_a_batch_query(keys, query)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize([num_b, num_t])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "def get_attention_score_for_a_batch_query(keys, query):\n",
    "  '''\n",
    "  This function returns a batch of attention score for each vector in (multi-batch) keys for a given (single-batch) query.\n",
    "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
    "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
    "  \n",
    "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
    "  \n",
    "  Arguments:\n",
    "    keys (torch.Tensor): Has a shape of [N, T, C]. These are vectors that a query wants attend to\n",
    "    query (torch.Tensor): Has a shape of [N, C]. This is a vector that attends to other set of vectors (keys and values)\n",
    "  \n",
    "  Output:\n",
    "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
    "                                    Has a shape of [N, T]\n",
    "                                    \n",
    "    attention_score[n, i] has to be a dot product value between keys[n, i] and query[n]                     \n",
    "    \n",
    "  TODO: Complete this function without using for loop\n",
    "  Hint: Use torch.bmm or torch.matmul after make two input tensors as 3-dim tensors.\n",
    "\n",
    "  '''\n",
    "  return \n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_b = 6\n",
    "num_t = 23\n",
    "h_size = 16\n",
    "\n",
    "keys = torch.randn(num_b,num_t, h_size)\n",
    "query = torch.randn(num_b, h_size)\n",
    "out = get_attention_score_for_a_batch_query(keys, query)\n",
    "\n",
    "assert out.ndim == 2 and out.shape == torch.Size([num_b, num_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ae30995b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  6.1462,   2.7917,   3.2981,  ...,  -1.7558,  -1.9945,   1.7817],\n",
       "         [ -2.0761,   1.5621,  -4.6314,  ...,  -2.9616,   5.0151,  -0.5098],\n",
       "         [ -3.7923,   0.6755,  -2.5517,  ...,  -6.6489,   2.2012,   0.5882],\n",
       "         ...,\n",
       "         [  2.9819,  12.6860,   6.7435,  ...,   3.5522,  -7.0258,   2.3800],\n",
       "         [ -5.4682,  -2.9139,  -0.3054,  ...,   6.4960,  -1.4581, -12.5525],\n",
       "         [ -1.0037,   1.1092,   1.3248,  ...,   2.8827,   3.8804,  -5.4968]],\n",
       "\n",
       "        [[ -2.9907,  -0.1470,  -0.1703,  ...,   2.4992,  -1.8304,   1.1768],\n",
       "         [-15.4518,   2.2430,   4.9486,  ...,   4.4271,  -4.3865,  -9.2907],\n",
       "         [  4.2723,  -0.6171,   2.6252,  ...,  -2.2281,  -2.5648,  -4.1481],\n",
       "         ...,\n",
       "         [  1.3169,  -1.1141,  -1.6058,  ...,   1.9466,   2.6665,  -4.1625],\n",
       "         [ -7.9481,   5.0494,   0.7725,  ...,   0.5016,  -3.3123,  -7.8802],\n",
       "         [  1.8112,  -3.9315,   1.6521,  ...,  -0.2215,  -0.1541,  -6.4050]],\n",
       "\n",
       "        [[ -3.0427,   1.6135,  -0.4640,  ...,   5.9792,   2.2480,  -3.4328],\n",
       "         [ -5.7212,  -8.7972,  -5.4037,  ...,   3.7540,   0.4384,  -3.4441],\n",
       "         [  1.0249,   4.4344,   1.3304,  ...,  -1.3935,   2.5567,  -3.9180],\n",
       "         ...,\n",
       "         [ 10.2565,   6.2974,  -2.4342,  ..., -10.0146,   3.6096,   1.5109],\n",
       "         [  9.3230,  -1.0268,  11.0852,  ...,  -3.6774,   4.8100,  -2.3779],\n",
       "         [  6.1441,  -4.8747,   0.2777,  ...,   7.1300,  -3.4380,   1.1166]],\n",
       "\n",
       "        [[ -5.6300,   1.6126,   2.5171,  ...,   3.5575,   3.1118,   2.0304],\n",
       "         [  2.6453,  -0.9799,   0.6216,  ...,  -3.8061,   3.2136,  -0.5925],\n",
       "         [  3.2414,   5.6337,   3.7929,  ...,   0.3125,   1.4103,  -5.6959],\n",
       "         ...,\n",
       "         [ -1.5721,   2.2379,   4.6053,  ...,  -1.0397,  -3.7134,  -4.8251],\n",
       "         [ -4.5574,   3.2444,   2.8786,  ...,   1.6313,   0.1190,  -1.0030],\n",
       "         [ -1.4957,   0.2640,  -2.1453,  ...,  -2.5968,   0.1317,   2.6204]],\n",
       "\n",
       "        [[ -4.5165,  11.9102,  -1.3893,  ...,  -4.5206,   1.6530,   9.5429],\n",
       "         [  0.5686,  -5.3511,   2.3664,  ...,  -4.7446,   5.1878,   1.1045],\n",
       "         [  6.2639,   0.1570,  -0.1588,  ...,   2.1696,  -5.3117,   3.4494],\n",
       "         ...,\n",
       "         [ -3.1651,  -8.5573,  -2.6017,  ...,  -2.2827,   2.3201, -11.6823],\n",
       "         [  8.1099,   3.2290,  -7.5239,  ...,  -1.7162,  -8.0600,  -0.5222],\n",
       "         [  1.5312,  -0.7118,   5.2198,  ...,   3.8161,  -3.3262,  -2.4121]],\n",
       "\n",
       "        [[  4.3940,  -1.3437,  -0.7749,  ...,   2.1454,  -2.6441,   0.9273],\n",
       "         [ -4.2766,   2.7691,  -5.5275,  ...,  10.0679,  -3.9916,  10.4126],\n",
       "         [ -0.5561,  -5.9345,  -0.2756,  ...,  -2.7162,   2.0791,  -0.3644],\n",
       "         ...,\n",
       "         [  0.8519,   1.8976,   0.7429,  ...,   0.3895,   5.3467,   2.4843],\n",
       "         [ -1.2661,   4.2320,  -1.3352,  ...,   0.2109,  -4.6615,   2.1014],\n",
       "         [ -2.1632,   1.4186,  -0.8282,  ...,   2.8492,  -1.6369,  -0.5276]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_attention_score_for_a_batch_multiple_query(keys, queries):\n",
    "  '''\n",
    "  Now you have to implement the attention score for not only single query, but multiple queries.\n",
    "  \n",
    "  This function returns a batch of attention score for each vector in keys for given queries.\n",
    "  You can regard 'keys' as hidden states over timestep of Encoder, while querys are hidden states over timestep of Decoder\n",
    "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
    "  \n",
    "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
    "  \n",
    "  Arguments:\n",
    "    keys (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that a query wants attend to\n",
    "    queries (torch.Tensor): Has a shape of [N, Tt, C]. This is a vector that attends to other set of vectors (keys and values)\n",
    "  \n",
    "  Output:\n",
    "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
    "                                    Has a shape of [N, Ts, Tt]\n",
    "                                    \n",
    "    attention_score[n, i, t] has to be a dot product value between keys[n, i] and query[n, t] \n",
    "    \n",
    "  TODO: Complete this function without using for loop\n",
    "  HINT: Use torch.bmm() with proper transpose (permutation) of given tensors. (You can use atensor.permute())\n",
    "        Think about which dimension (axis) of tensors has to be multiplied together and resolved (disappear) after matrix multiplication,\n",
    "        and how the result tensor has to look like (shape)\n",
    "  '''\n",
    "  return\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_b = 6\n",
    "num_ts = 23\n",
    "num_tt = 14\n",
    "h_size = 16\n",
    "\n",
    "keys = torch.randn(num_b, num_ts, h_size)\n",
    "queries = torch.randn(num_b, num_tt, h_size)\n",
    "att_score = get_attention_score_for_a_batch_multiple_query(keys, queries)\n",
    "\n",
    "att_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1cb45f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all the cases!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test cases\n",
    "'''\n",
    "answer = torch.Tensor([ 4.9620, -9.6091, -4.9472,  1.4543, -5.6273,  9.1436,  1.4172,  0.0464,\n",
    "        -5.7033,  4.5473,  7.7498,  1.3405, -3.1877,  2.8759])\n",
    "answer2 = torch.Tensor([[ 2.5171,  0.6216,  3.7929,  2.6163,  5.3290,  0.3592,  2.3067, -0.1099,\n",
    "         1.8963,  0.4175, -1.4283,  1.4388, -2.7825, -1.3690, -1.9615, -1.9514,\n",
    "        -6.4635,  1.9574,  0.1868,  8.5354,  4.6053,  2.8786, -2.1453]])\n",
    "assert att_score.ndim == 3 and att_score.shape == torch.Size([num_b, num_ts, num_tt]), 'Check the output shape'\n",
    "assert torch.max(torch.abs(att_score[2,4] - answer)) < 1e-4, 'Calculated result is wrong'\n",
    "assert torch.max(torch.abs(att_score[3,:,2] - answer2)) < 1e-4,  'Calculated result is wrong'\n",
    "\n",
    "print(\"Passed all the cases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "19ef2611",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    0.0851,     0.0001,     0.0264,  ...,     0.0002,\n",
       "              0.0000,     0.0015],\n",
       "         [    0.0000,     0.0000,     0.0000,  ...,     0.0001,\n",
       "              0.0464,     0.0001],\n",
       "         [    0.0000,     0.0000,     0.0001,  ...,     0.0000,\n",
       "              0.0028,     0.0004],\n",
       "         ...,\n",
       "         [    0.0036,     0.9961,     0.8280,  ...,     0.0376,\n",
       "              0.0000,     0.0026],\n",
       "         [    0.0000,     0.0000,     0.0007,  ...,     0.7145,\n",
       "              0.0001,     0.0000],\n",
       "         [    0.0001,     0.0000,     0.0037,  ...,     0.0193,\n",
       "              0.0149,     0.0000]],\n",
       "\n",
       "        [[    0.0000,     0.0001,     0.0000,  ...,     0.0722,\n",
       "              0.0003,     0.0022],\n",
       "         [    0.0000,     0.0008,     0.0014,  ...,     0.4965,\n",
       "              0.0000,     0.0000],\n",
       "         [    0.0021,     0.0000,     0.0001,  ...,     0.0006,\n",
       "              0.0001,     0.0000],\n",
       "         ...,\n",
       "         [    0.0001,     0.0000,     0.0000,  ...,     0.0416,\n",
       "              0.0233,     0.0000],\n",
       "         [    0.0000,     0.0131,     0.0000,  ...,     0.0098,\n",
       "              0.0001,     0.0000],\n",
       "         [    0.0002,     0.0000,     0.0001,  ...,     0.0048,\n",
       "              0.0014,     0.0000]],\n",
       "\n",
       "        [[    0.0000,     0.0048,     0.0000,  ...,     0.0103,\n",
       "              0.0011,     0.0001],\n",
       "         [    0.0000,     0.0000,     0.0000,  ...,     0.0011,\n",
       "              0.0002,     0.0001],\n",
       "         [    0.0001,     0.0808,     0.0001,  ...,     0.0000,\n",
       "              0.0015,     0.0001],\n",
       "         ...,\n",
       "         [    0.6014,     0.5207,     0.0000,  ...,     0.0000,\n",
       "              0.0044,     0.0184],\n",
       "         [    0.2364,     0.0003,     0.9970,  ...,     0.0000,\n",
       "              0.0147,     0.0004],\n",
       "         [    0.0098,     0.0000,     0.0000,  ...,     0.0326,\n",
       "              0.0000,     0.0124]],\n",
       "\n",
       "        [[    0.0000,     0.0014,     0.0022,  ...,     0.1161,\n",
       "              0.1248,     0.0004],\n",
       "         [    0.0013,     0.0001,     0.0003,  ...,     0.0001,\n",
       "              0.1382,     0.0000],\n",
       "         [    0.0024,     0.0794,     0.0080,  ...,     0.0045,\n",
       "              0.0228,     0.0000],\n",
       "         ...,\n",
       "         [    0.0000,     0.0027,     0.0181,  ...,     0.0012,\n",
       "              0.0001,     0.0000],\n",
       "         [    0.0000,     0.0073,     0.0032,  ...,     0.0169,\n",
       "              0.0063,     0.0000],\n",
       "         [    0.0000,     0.0004,     0.0000,  ...,     0.0002,\n",
       "              0.0063,     0.0008]],\n",
       "\n",
       "        [[    0.0000,     0.9994,     0.0001,  ...,     0.0000,\n",
       "              0.0206,     0.9403],\n",
       "         [    0.0009,     0.0000,     0.0032,  ...,     0.0000,\n",
       "              0.7063,     0.0002],\n",
       "         [    0.2632,     0.0000,     0.0003,  ...,     0.0029,\n",
       "              0.0000,     0.0021],\n",
       "         ...,\n",
       "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
       "              0.0000,     0.0000],\n",
       "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
       "              0.0000,     0.0000],\n",
       "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
       "              0.0000,     0.0000]],\n",
       "\n",
       "        [[    0.2580,     0.0001,     0.0059,  ...,     0.0003,\n",
       "              0.0003,     0.0001],\n",
       "         [    0.0000,     0.0044,     0.0001,  ...,     0.9595,\n",
       "              0.0001,     0.9706],\n",
       "         [    0.0018,     0.0000,     0.0097,  ...,     0.0000,\n",
       "              0.0348,     0.0000],\n",
       "         ...,\n",
       "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
       "              0.0000,     0.0000],\n",
       "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
       "              0.0000,     0.0000],\n",
       "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
       "              0.0000,     0.0000]]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_masked_softmax(attention_score, mask, mask_value=-1e10):\n",
    "  '''\n",
    "  During the batch computation, each sequence in the batch can have different length.\n",
    "  To group them as in a single tensor, we usually pad values\n",
    "    \n",
    "  Arguments:\n",
    "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
    "                                    Has a shape of [N, Ts, Tt]\n",
    "    mask (torch.Tensor): Boolean tensor with a shape of [N, Ts] that represents whether the corresponding is valid or not.\n",
    "                         mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
    "                         If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
    "  \n",
    "  Output:\n",
    "    attention_weight (torch.Tensor): The attention weight in real number between 0 and 1. The sum of attention_weight along keys timestep dimension is 1.\n",
    "                                    Has a shape of [N, Ts, Tt]\n",
    "                                    \n",
    "    attention_weight[n, i, t] has to be an attention weight of values[n, i] for queries[n, t] \n",
    "    \n",
    "  TODO: Complete this function without using for loop\n",
    "  '''\n",
    "\n",
    "  return\n",
    "\n",
    "\n",
    "'''\n",
    "Don't change this codes\n",
    "'''\n",
    "mask = torch.ones_like(att_score)[..., 0]\n",
    "mask[4, 15:] = 0\n",
    "mask[5, 17:] = 0\n",
    "\n",
    "attention_weight = get_masked_softmax(att_score, mask)\n",
    "attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99abd97a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m answer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0.0120\u001b[39m,     \u001b[38;5;241m0.0002\u001b[39m,     \u001b[38;5;241m0.0901\u001b[39m,     \u001b[38;5;241m0.0003\u001b[39m,     \u001b[38;5;241m0.0259\u001b[39m,     \u001b[38;5;241m0.0036\u001b[39m,\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;241m0.5617\u001b[39m,     \u001b[38;5;241m0.0108\u001b[39m,     \u001b[38;5;241m0.2508\u001b[39m,     \u001b[38;5;241m0.0054\u001b[39m,     \u001b[38;5;241m0.0001\u001b[39m,     \u001b[38;5;241m0.0010\u001b[39m,\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;241m0.0000\u001b[39m,     \u001b[38;5;241m0.0005\u001b[39m,     \u001b[38;5;241m0.0375\u001b[39m,     \u001b[38;5;241m0.0000\u001b[39m,     \u001b[38;5;241m0.0000\u001b[39m,     \u001b[38;5;241m0.0000\u001b[39m,\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;241m0.0000\u001b[39m,     \u001b[38;5;241m0.0000\u001b[39m,     \u001b[38;5;241m0.0000\u001b[39m,     \u001b[38;5;241m0.0000\u001b[39m,     \u001b[38;5;241m0.0000\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(\u001b[43mattention_weight\u001b[49m[\u001b[38;5;241m4\u001b[39m,:,\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m-\u001b[39manswer)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (attention_weight\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m )\u001b[38;5;241m.\u001b[39mall()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attention_weight' is not defined"
     ]
    }
   ],
   "source": [
    "answer = torch.Tensor([0.0120,     0.0002,     0.0901,     0.0003,     0.0259,     0.0036,\n",
    "            0.5617,     0.0108,     0.2508,     0.0054,     0.0001,     0.0010,\n",
    "            0.0000,     0.0005,     0.0375,     0.0000,     0.0000,     0.0000,\n",
    "            0.0000,     0.0000,     0.0000,     0.0000,     0.0000])\n",
    "assert torch.max(torch.abs(attention_weight[4,:,3]-answer)) < 1e-4\n",
    "assert torch.max(torch.abs(attention_weight.sum(1) -  1 )) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2e5c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m  This function converts attention score to attention weight\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m  \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m  TODO: Complete this function using torch.mm\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  '''\u001b[39;00m\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbmm(attention_weight\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m), values)\n\u001b[0;32m---> 19\u001b[0m att_out \u001b[38;5;241m=\u001b[39m get_batch_weighted_sum(keys, \u001b[43mattention_weight\u001b[49m)\n\u001b[1;32m     20\u001b[0m att_out\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attention_weight' is not defined"
     ]
    }
   ],
   "source": [
    "def get_batch_weighted_sum(values, attention_weight):\n",
    "  '''\n",
    "  This function converts attention score to attention weight\n",
    "  \n",
    "  Argument:\n",
    "    values (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that are used to form attention vector\n",
    "    attention_weight: Has a shape of [N, Ts, Tt], which represents the weight for each vector to compose the attention vector\n",
    "                      attention_weight[n, s, t] represents weight for value[n, s] that corresponds to a given query, queries[n, t]\n",
    "\n",
    "  Output:\n",
    "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight. \n",
    "                                     Has a shape of [N, Tt, C]\n",
    "  \n",
    "  TODO: Complete this function using torch.mm\n",
    "  '''\n",
    "  \n",
    "  return\n",
    "\n",
    "att_out = get_batch_weighted_sum(keys, attention_weight)\n",
    "att_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93231ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test cases\n",
    "'''\n",
    "answer = torch.Tensor([ 4.9620, -9.6091, -4.9472,  1.4543, -5.6273,  9.1436,  1.4172,  0.0464,\n",
    "        -5.7033,  4.5473,  7.7498,  1.3405, -3.1877,  2.8759])\n",
    "answer2 = torch.Tensor([[ 2.5171,  0.6216,  3.7929,  2.6163,  5.3290,  0.3592,  2.3067, -0.1099,\n",
    "         1.8963,  0.4175, -1.4283,  1.4388, -2.7825, -1.3690, -1.9615, -1.9514,\n",
    "        -6.4635,  1.9574,  0.1868,  8.5354,  4.6053,  2.8786, -2.1453]])\n",
    "assert att_score.ndim == 3 and att_score.shape == torch.Size([num_b, num_ts, num_tt]), 'Check the output shape'\n",
    "assert torch.max(torch.abs(att_score[2,4] - answer)) < 1e-4, 'Calculated result is wrong'\n",
    "assert torch.max(torch.abs(att_score[3,:,2] - answer2)) < 1e-4,  'Calculated result is wrong'\n",
    "\n",
    "print(\"Passed all the cases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbdbf2b",
   "metadata": {},
   "source": [
    "## Problem 3: Make seq2seq with attention\n",
    "- Using Pre-defined `TranslatorBi` class, complete a new `TranslaterAtt` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c670709",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download dataset (originally from NIA AI-Hub)\n",
    "'''\n",
    "\n",
    "!gdown 1CpsqOuuuB3I_PG5DbuqH1ssCFVerU46g\n",
    "!unzip -q nia-aihub-korean-english.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path('aat3020-2022Spring/nia_korean_english')\n",
    "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
    " \n",
    "# Use only first two xlsx files in the assignment\n",
    "data_list = data_list[:2]\n",
    "df = pd.concat([pd.read_excel(path) for path in data_list], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093978a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in data_list:\n",
    "  df = pd.read_excel(path)\n",
    "  kor_text_path = path.parent / (path.stem+'_kor.txt') \n",
    "  eng_text_path = path.parent / (path.stem+'_eng.txt') \n",
    "  with open(kor_text_path, 'w', encoding='utf8') as f:\n",
    "      f.write('\\n'.join(df['원문']))\n",
    "  with open(eng_text_path, 'w', encoding='utf8') as f:\n",
    "      f.write('\\n'.join(df['번역문']))\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "# Train Tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
    "\n",
    "vocab_size    = 32000  # Number of maximum size of the vocabulary\n",
    "limit_alphabet= 6000   \n",
    "min_frequency = 5 \n",
    "\n",
    "corpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list]\n",
    "output_dir   = Path('hugging_kor_partial_%d'%(vocab_size))\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet, \n",
    "               show_progress=True)\n",
    "\n",
    "tokenizer.save_model(str(output_dir))\n",
    "\n",
    "limit_alphabet= 200\n",
    "corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list]\n",
    "output_dir   = Path('hugging_eng_partial_%d'%(vocab_size))\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet, \n",
    "               show_progress=True)\n",
    "\n",
    "tokenizer.save_model(str(output_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7061145",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
    "                                                       strip_accents=False,\n",
    "                                                       lowercase=False) \n",
    "tgt_tokenizer = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
    "                                                       strip_accents=False,\n",
    "                                                       lowercase=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fa529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationSet:\n",
    "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
    "    self.data = df[ ['원문', '번역문']].values\n",
    "    self.src_tokenizer = src_tokenizer\n",
    "    self.tgt_tokenizer = tgt_tokenizer\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    src_str = self.data[idx, 0]\n",
    "    tgt_str = self.data[idx, 1]\n",
    "\n",
    "    # convert string to list of token ids\n",
    "    src_ids = self.src_tokenizer.encode(src_str)\n",
    "    tgt_ids = self.tgt_tokenizer.encode(tgt_str)\n",
    "\n",
    "    return torch.LongTensor(src_ids), torch.LongTensor(tgt_ids) # idx-th datasample\n",
    "  \n",
    "entireset = TranslationSet(df, src_tokenizer, tgt_tokenizer)\n",
    "trainset, validset, testset = torch.utils.data.random_split(entireset, [int(len(entireset)*0.9), int(len(entireset)*0.05), len(entireset)-int(len(entireset)*0.9)-int(len(entireset)*0.05)], generator=torch.Generator().manual_seed(42))\n",
    "# trainset, validset, testset = torch.utils.data.random_split(entireset, [360000, 20000, 20000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f'Dataset Item Example: {entireset[0]}')\n",
    "print(f'Length of split : Train {len(trainset)}, Valid {len(validset)}, Test {len(testset)}')\n",
    "\n",
    "def pack_collate(raw_batch):\n",
    "  srcs = [x[0] for x in raw_batch]\n",
    "  tgts_i = [x[1][:-1] for x in raw_batch]\n",
    "  tgts_o = [x[1][1:] for x in raw_batch]\n",
    "  \n",
    "  srcs = pack_sequence(srcs, enforce_sorted=False)\n",
    "  tgts_i = pack_sequence(tgts_i, enforce_sorted=False)\n",
    "  tgts_o = pack_sequence(tgts_o, enforce_sorted=False)\n",
    "  return srcs, tgts_i, tgts_o\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=64, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-defined class\n",
    "'''\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device, model_name='nmt_model'):\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_fn = loss_fn\n",
    "    self.train_loader = train_loader\n",
    "    self.valid_loader = valid_loader\n",
    "    \n",
    "    self.model.to(device)\n",
    "    \n",
    "    self.grad_clip = 3\n",
    "    self.best_valid_accuracy = 0\n",
    "    self.device = device\n",
    "    \n",
    "    self.training_loss = []\n",
    "    self.validation_loss = []\n",
    "    self.validation_acc = []\n",
    "    self.model_name = model_name\n",
    "\n",
    "  def save_model(self, path):\n",
    "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
    "    \n",
    "  def train_by_num_epoch(self, num_epochs):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "      self.model.train()\n",
    "      for batch in tqdm(self.train_loader, leave=False):\n",
    "        loss_value = self._train_by_single_batch(batch)\n",
    "        self.training_loss.append(loss_value)\n",
    "      self.model.eval()\n",
    "      validation_loss, validation_acc = self.validate()\n",
    "      self.validation_loss.append(validation_loss)\n",
    "      self.validation_acc.append(validation_acc)\n",
    "      \n",
    "      if validation_acc > self.best_valid_accuracy:\n",
    "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
    "        self.save_model(f'{self.model_name}_best.pt')\n",
    "      else:\n",
    "        self.save_model(f'{self.model_name}_last.pt')\n",
    "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
    "\n",
    "      \n",
    "  def _train_by_single_batch(self, batch):\n",
    "    '''\n",
    "    This method updates self.model's parameter with a given batch\n",
    "    \n",
    "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
    "    \n",
    "    You have to use variables below:\n",
    "    \n",
    "    self.model (Translator/torch.nn.Module): A neural network model\n",
    "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
    "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
    "    self.device (str): 'cuda' or 'cpu'\n",
    "\n",
    "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
    "    The model's parameters, optimizer's steps has to be updated inside this method\n",
    "    '''\n",
    "    \n",
    "    src, tgt_i, tgt_o = batch\n",
    "    pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
    "    loss = self.loss_fn(pred.data, tgt_o.data)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "    \n",
    "  def validate(self, external_loader=None):\n",
    "    '''\n",
    "    This method calculates accuracy and loss for given data loader.\n",
    "    It can be used for validation step, or to get test set result\n",
    "    \n",
    "    input:\n",
    "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
    "      \n",
    "    output: \n",
    "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
    "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
    "    '''\n",
    "    \n",
    "    ### Don't change this part\n",
    "    if external_loader and isinstance(external_loader, DataLoader):\n",
    "      loader = external_loader\n",
    "      print('An arbitrary loader is used instead of Validation loader')\n",
    "    else:\n",
    "      loader = self.valid_loader\n",
    "      \n",
    "    self.model.eval()\n",
    "    \n",
    "    '''\n",
    "    Write your code from here, using loader, self.model, self.loss_fn.\n",
    "    '''\n",
    "    validation_loss = 0\n",
    "    validation_acc = 0\n",
    "    num_total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "      for batch in tqdm(loader, leave=False):\n",
    "        \n",
    "        src, tgt_i, tgt_o = batch\n",
    "        pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
    "        loss = self.loss_fn(pred.data, tgt_o.data)\n",
    "        num_tokens = tgt_i.data.shape[0]\n",
    "        validation_loss += loss.item() * num_tokens\n",
    "        num_total_tokens += num_tokens\n",
    "        \n",
    "        acc = torch.sum(torch.argmax(pred.data, dim=-1) == tgt_o.to(self.device).data)\n",
    "        validation_acc += acc.item()\n",
    "        \n",
    "    return validation_loss / num_total_tokens, validation_acc / num_total_tokens\n",
    "\n",
    "def get_cross_entropy_loss(predicted_prob_distribution, indices_of_correct_token, eps=1e-7):\n",
    "  '''\n",
    "  for PackedSequence, the input is 2D tensor\n",
    "  \n",
    "  predicted_prob_distribution has a shape of [num_entire_tokens_in_the_batch x vocab_size]\n",
    "  indices_of_correct_token has a shape of [num_entire_tokens_in_the_batch]\n",
    "  '''\n",
    "  prob_of_correct_next_word = predicted_prob_distribution[torch.arange(len(predicted_prob_distribution)), indices_of_correct_token]\n",
    "  loss = -torch.log(prob_of_correct_next_word + eps)\n",
    "  return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "11a011e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-defined class\n",
    "'''\n",
    "class TranslatorBi(nn.Module):\n",
    "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=256, num_layers=3):\n",
    "    super().__init__()\n",
    "    self.src_tokenizer = src_tokenizer\n",
    "    self.tgt_tokenizer = tgt_tokenizer\n",
    "    \n",
    "    self.src_vocab_size = self.src_tokenizer.vocab_size\n",
    "    self.tgt_vocab_size = self.tgt_tokenizer.vocab_size\n",
    "    \n",
    "    self.src_embedder = nn.Embedding(self.src_vocab_size, hidden_size)\n",
    "    self.tgt_embedder = nn.Embedding(self.tgt_vocab_size, hidden_size)\n",
    "    \n",
    "    self.encoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "    self.decoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "    \n",
    "    self.decoder_proj = nn.Linear(hidden_size, self.tgt_vocab_size)\n",
    "    \n",
    "  def run_encoder(self, x):\n",
    "    if isinstance(x, PackedSequence):\n",
    "      emb_x = PackedSequence(self.src_embedder(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
    "    else:\n",
    "      emb_x = self.src_embedder(x)\n",
    "      \n",
    "    enc_hidden_state_by_t, last_hidden = self.encoder(emb_x)\n",
    "    \n",
    "    # Because we use bi-directional GRU, there are (num_layers * 2) last hidden states\n",
    "    # Here, we make it to (num_layers) last hidden states by taking mean of [left-to-right-GRU] and [right-to-left-GRU]\n",
    "    last_hidden_sum = last_hidden.reshape(self.encoder.num_layers, 2, last_hidden.shape[1], -1).mean(dim=1)\n",
    "    if isinstance(x, PackedSequence):\n",
    "      hidden_mean = enc_hidden_state_by_t.data.reshape(-1, 2, last_hidden_sum.shape[-1]).mean(1)\n",
    "      enc_hidden_state_by_t = PackedSequence(hidden_mean, x[1], x[2], x[3])\n",
    "    else:\n",
    "      enc_hidden_state_by_t = enc_hidden_state_by_t.reshape(x.shape[0], x.shape[1], 2, -1).mean(dim=2)\n",
    "      \n",
    "    \n",
    "    return enc_hidden_state_by_t, last_hidden_sum \n",
    "\n",
    "  def run_decoder(self, y, last_hidden_state):\n",
    "    if isinstance(y, PackedSequence):\n",
    "      emb_y = PackedSequence(self.tgt_embedder(y.data), batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
    "    else:\n",
    "      emb_y = self.src_embedder(y)\n",
    "    out, decoder_last_hidden = self.decoder(emb_y, last_hidden_state)\n",
    "    return out, decoder_last_hidden\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    '''\n",
    "    x (torch.Tensor or PackedSequence): Batch of source sentences\n",
    "    y (torch.Tensor or PackedSequence): Batch of target sentences\n",
    "    '''\n",
    "    \n",
    "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
    "    out, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
    "    \n",
    "    if isinstance(out, PackedSequence):\n",
    "      logits = self.decoder_proj(out.data)\n",
    "      probs = torch.softmax(logits, dim=-1)\n",
    "      probs = PackedSequence(probs, batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
    "    else:\n",
    "      logits = self.decoder_proj(out)\n",
    "      probs = torch.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060d164",
   "metadata": {},
   "source": [
    "### Problem: Complete the Seq2Seq with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9e285f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
       "             0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
       "             0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
       "             0.0000],\n",
       "        ...,\n",
       "        [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
       "             0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
       "             0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
       "             0.0000]], grad_fn=<SoftmaxBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 63, 63, 62, 60, 55, 54, 51, 47, 45, 44, 42,\n",
       "        40, 38, 37, 36, 34, 33, 31, 29, 28, 28, 27, 27, 26, 23, 23, 23, 23, 20,\n",
       "        20, 19, 18, 17, 15, 14, 14, 13, 13, 11,  8,  7,  5,  4,  2,  1]), sorted_indices=tensor([ 5, 51, 14,  8, 24, 31, 10, 47, 55, 62, 17, 50, 13,  6,  4, 20, 46,  9,\n",
       "        49, 21, 37, 23, 22, 60, 18, 58, 28, 36, 19, 34,  1, 30,  0, 11, 52, 15,\n",
       "        16, 29, 56, 42, 33, 48, 40, 43, 54, 63, 35,  2, 39, 27, 57, 61, 53, 12,\n",
       "         3, 26, 59, 38, 41, 44, 25, 45,  7, 32]), unsorted_indices=tensor([32, 30, 47, 54, 14,  0, 13, 62,  3, 17,  6, 33, 53, 12,  2, 35, 36, 10,\n",
       "        24, 28, 15, 19, 22, 21,  4, 60, 55, 49, 26, 37, 31,  5, 63, 40, 29, 46,\n",
       "        27, 20, 57, 48, 42, 58, 39, 43, 59, 61, 16,  7, 41, 18, 11,  1, 34, 52,\n",
       "        44,  8, 38, 50, 25, 56, 23, 51,  9, 45]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TranslaterAtt(TranslatorBi):\n",
    "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=512, num_layers=3):\n",
    "    super().__init__(src_tokenizer, tgt_tokenizer, hidden_size, num_layers)\n",
    "    \n",
    "    # TODO: define new self.decoder_proj\n",
    "    self.decoder_proj = nn.Linear(hidden_size * 2, self.tgt_vocab_size)\n",
    "    \n",
    "  def get_attention_vector(self, encoder_hidden_states, decoder_hidden_states, mask):\n",
    "    '''\n",
    "    Arguments:\n",
    "      x (torch.Tensor or PackedSequence)\n",
    "      y (torch.Tensor or PackedSequence)\n",
    "    Outputs:\n",
    "      attention_vectors (torch.Tensor or PackedSequence)\n",
    "    \n",
    "    TODO: Complete this function\n",
    "    If the inputs are PackedSequence, the output has to be a PackedSequence\n",
    "    Use torch.nn.utils.rnn.pad_packed_sequence(packed_sequence, batch_first=True)\n",
    "    '''\n",
    "    is_packed = isinstance(encoder_hidden_states, PackedSequence)\n",
    "    \n",
    "  \n",
    "  def forward(self, x, y):\n",
    "    '''\n",
    "    Arguments:\n",
    "      x (torch.Tensor or PackedSequence): Batch of source sentences\n",
    "      y (torch.Tensor or PackedSequence): Batch of target sentences\n",
    "    Output:\n",
    "      prob_dist (torch.Tensor or PackedSequence): Batch of probability distribution of word for target sentence\n",
    "    \n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "\n",
    "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
    "    dec_hidden_state_by_t, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
    "    \n",
    "    mask = pad_packed_sequence(x, batch_first=True)[0] == 0\n",
    "    attention_vec = self.get_attention_vector(enc_hidden_state_by_t, dec_hidden_state_by_t, mask)\n",
    "    # Write your code from here\n",
    "    return\n",
    "model = TranslaterAtt(src_tokenizer, tgt_tokenizer, hidden_size=32, num_layers=2)\n",
    "\n",
    "model(batch[0], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test Case for Single-size Batch\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test Case for Batch with PackedSequence\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c0741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c783f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7292462",
   "metadata": {},
   "source": [
    "## Problem 4: Self Attention (Under Construction)\n",
    "- In this problem, you will implement the attention algorithm that is used for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "35fb7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_query_value(input_tensor, kqv_layer):\n",
    "  '''\n",
    "  This function returns key, query, and value that is calculated by input tensor and nn_layer.\n",
    "\n",
    "  Arguments:\n",
    "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
    "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
    "    \n",
    "  Outputs:\n",
    "    keys (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "    queries (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "    values (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "    \n",
    "  Hint: Use torch.chunk() to split a tensor into given number of chunks\n",
    "  '''\n",
    "  return \n",
    "\n",
    "torch.manual_seed(0)\n",
    "test = torch.randn(4, 17, 8)\n",
    "linear = nn.Linear(8, 16 * 3)\n",
    "keys, queries, values = get_key_query_value(test, linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "82c0cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test cases\n",
    "'''\n",
    "assert keys.ndim == queries.ndim == values.ndim == 3\n",
    "assert keys.shape == queries.shape == values.shape == torch.Size([4, 17, 16])\n",
    "assert not (keys==queries).any() and not (keys==values).any() and not (values==queries).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7dc503bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_attention(input_tensor, kqv_layer, mask):\n",
    "  '''\n",
    "  This function returns output of self-attention for a given input tensor using with a given kqv_layer\n",
    "  \n",
    "  Arguments:\n",
    "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
    "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
    "    mask (torch.Tensor): \n",
    "    \n",
    "  Outputs:\n",
    "    output (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "\n",
    "  TODO: Complete this function using your completed functions of below:\n",
    "        get_attention_score_for_a_batch_multiple_query()\n",
    "        get_masked_softmax()\n",
    "        get_batch_weighted_sum()\n",
    "        get_key_query_value()\n",
    "  '''\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f15c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d119d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e9278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
